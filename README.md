# Blue_team_MechanisticInterpretability
The goal of mechanistic interpretability is to take a trained model and reverse engineer the algorithms the model learned during training from its weights . we have no idea how they work nor how to write one ourselves . <br>
So our project is to Investigating Agent Behavior In different Reinforcement Learning methods by optimizing strategy to gain as much reward as possible. <br>
This  <a href="https://github.com/Mohammed20201991/Blue_team_MechanisticInterpretability/blob/main/FinalSubmetion.ipynb" >notebook </a> presents methods for pruning different Reinforcement 
Learning algorithms that show different agent behavior in different models, to facilitate research into understanding the behavior of these strategies. Implementing and vitalizing Dynamic Programming, Monte Carlo (MC), and Temporal difference (TD) algorithms and comparing their results. The pruning algorithm takes a given dataset that has 2 cases to show the shortest path depending on how the agent learns from their behavior where probability are given .
<h5>Algorithms model and non-model based </h5>
-  1: Policy & Value Iteration <br>
-  2: Monte Carlo  <br>
-  3: Temporal Difference <br>

<h3> How to use is ?</h3>
Download the notebook and you can run it using Jupyter or Google collab 

<h3> Dataset</h3>
Set your correct path to upload dataset .
Data was genereted randomly you can choose any row by ID(Neptune) which has 2 cases 
feel free to play with it and see how the agent behave .<br>
<a href="https://github.com/Mohammed20201991/Blue_team_MechanisticInterpretability/blob/main/data/input_data.csv" > data</a>
